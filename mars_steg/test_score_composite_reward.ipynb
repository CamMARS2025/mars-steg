{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def reward(\n",
    "    task_score: float,\n",
    "    language_score: float,\n",
    "    t_weight: float = 1.0,\n",
    "    l_weight: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the harmonic weighted reward score based on task success and language aspect.\n",
    "\n",
    "    This method calculates a reward by combining the task performance score \n",
    "    with a penalty based on the language aspect. The final score is normalized \n",
    "    to the range [0, 1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    task_score : float\n",
    "        A score in the range [0, 1] indicating task performance.\n",
    "    language_score : float\n",
    "        A score in the range [0, 1] indicating adherence to language constraints. It equals in our cases often to 1 - penalisation_score, where penalisation_score is included between 0 and 1.\n",
    "    t_weight : float, optional\n",
    "        The weight assigned to the task score (default is 1.0).\n",
    "    l_weight : float, optional\n",
    "        The weight assigned to the language score penalty (default is 1.0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A normalized reward score in the range [0, 1].\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - First implementation was using a weighted average score, which induced the following issue : a random agent would have a constant reward of 0.5, whereas , in pricing game, a failing calculation of bid additioned to using P1 information was leading to a reward of 0. \n",
    "    In math task example, saying random things without using equal sign was leading as well to a reward 0.5, where trying to solve the problem was inducing a worst reward. We hope to fix this problematic case with a weighted harmonic reward.\n",
    "    - A higher `task_score` increases the reward.\n",
    "    - A higher `language_score` decreases the reward.\n",
    "    - A low task score or language score induces a low reward.\n",
    "    - Custom reward functions can be implemented by overriding this method.\n",
    "    \"\"\"\n",
    "    r = (t_weight + l_weight) * task_score * language_score / (t_weight * language_score + l_weight * task_score + torch.finfo(torch.float32).eps)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4615384192156407"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward(0.3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer(\"\"\"I need to determine the total cost of the product, including all factors provided.\n",
    "The procurement department negotiated a base material cost of $23.735 per unit. Additionally, overhead costs add 31.445% to production costs. There's also a volume incentive of 2.742% per unit. \n",
    "\n",
    "I'll start by calculating the overhead cost. I'll take the base material cost and multiply it by 31.445% to find the additional overhead. Then, I'll add this amount to the base material cost to get the total production cost.\n",
    "\n",
    "Next, I'll include the volume incentive. I'll take the total production cost and add 2.742% of that cost to account for the volume incentive.\n",
    "\n",
    "Finally, I'll add the profit margin of 28% to the adjusted price to determine the final price I will bid. This final price will be the amount I will offer for the product.\"\"\", add_special_tokens=False)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mars_steg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
