Model:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"  # The model name
  model_save_path: "./experiments/models"  # The path to save the model
  load_precision_mode: "8bits" # Options: ["4bits","8bits","full"]
  lora: true # false
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_bias: "none" #sticking to example : https://huggingface.co/docs/trl/main/en/peft_integration

Optimizer:
  optimizer_name: "adam"
  learning_rate: 5.0e-5
  weight_decay: 0.01
  adam_epsilon: 1e-8
  warmup_steps: 500
  max_grad_norm: 1.0

Train:
  log_with: null  # Use 'wandb' to log with wandb
  mini_batch_size: 1  # The PPO minibatch size
  batch_size: 8 # The batch size
  gradient_accumulation_steps: 4  # The number of gradient accumulation steps
  seed: 42  # The seed for reproducibility
  num_train_epochs: 5  # The number of training epochs
  num_eval_episodes: 10  # The number of evaluation episodes
  save_frequency: 50  # The frequency to save the model relative to the number of training data batches
  ppo_epochs: 10  # The number of PPO epochs
  train_proportion : 0.95  # The proportion of the training data
  validation_proportion: 0.05  # The proportion of the validation data (out of left data after training data split)
  number_shared_layers_for_ref_model: 0 # The number of shared layers for the reference model in PPO RLHF
  create_ref_model: False   # Whether to create a reference model in PPO RLHF
  evaluate_cot_gap: False  # Whether to evaluate the COT gap
  # cot_gap_evaluation_number_batches_samples: 1  # The number of batches sampled for evaluating the COT gap
  number_of_evaluations: 0  # The number of evaluations for the COT gap among epochs
  skipping_failed_parsing_examples: False #Skip examples that failed when parsing them for task score, language score, answer extraction, CoT extrcation etc
  run_optimization: True
  curriculum_threshold : 0.8 # Threshold indicating whether to decrease max_token_length when using c_length_penalisation
  max_length_decreasing_rate: 0.1 #Rate of decreasing max_token_length when using c_length_penalisation
  is_peft_model: True # Wheter we are using peft model
  init_kl_coef: 0.05 #float value to initiate the kl coef (constant for adap_kl_ctrl: False or init value for adaptative KL otherwise)
  adap_kl_ctrl: False # wheter to use a constant or adaptative KL control
  lora_unload_measurement : False # wheter to use the lora unloading measurement
  penalisation_class_kwargs: {}

Generation:
  min_length: 0
  # top_k: 0.0
  top_p: 0.9
  num_beams: 2
  temperature: 0.6
  do_sample: true
  pad_token_id: 0
  max_new_tokens: 700
  return_prompt: False
  generate_ref_response: False
  bos_token_id: 128000
  eos_token_id:
    - 128001
    - 128008
    - 128009
  use_cache : True
