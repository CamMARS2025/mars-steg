Model:
  model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"  # The model name
  model_save_path: "./experiments/models"  # The path to save the model
  load_precision_mode: "8bits" # Options: ["4bits","8bits","full"]
  lora: true # false
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_bias: "none" #sticking to example : https://huggingface.co/docs/trl/main/en/peft_integration

Optimizer:
  optimizer_name: "adam"
  learning_rate: 5.0e-5
  weight_decay: 0.01
  adam_epsilon: 1e-8
  warmup_steps: 500
  max_grad_norm: 1.0

Train:
  log_with: null  # Use 'wandb' to log with wandb
  mini_batch_size: 1  # The PPO minibatch size
  batch_size: 2 # The batch size
  gradient_accumulation_steps: 1  # The number of gradient accumulation steps
  seed: 42  # The seed for reproducibility
  num_train_epochs: 10  # The number of training epochs
  num_eval_episodes: 10  # The number of evaluation episodes
  save_frequency: 100  # The frequency to save the model relative to the number of training data batches
  ppo_epochs: 1  # The number of PPO epochs
  train_proportion : 0.95  # The proportion of the training data
  validation_proportion : 0.05  # The proportion of the validation data (out of left data after training data split)
  number_shared_layers_for_ref_model: 30 # The number of shared layers for the reference model in PPO RLHF
  create_ref_model: False   # Whether to create a reference model in PPO RLHF
  evaluate_cot_gap: False  # Whether to evaluate the COT gap
  # cot_gap_evaluation_number_batches_samples: 1  # The number of batches sampled for evaluating the COT gap
  number_of_evaluations: 3  # The number of evaluations for the COT gap among epochs
  skipping_failed_parsing_examples: False #Skip examples that failed when parsing them for task score, language score, answer extraction, CoT extrcation etc
  run_optimization: True

Generation:
  min_length: 0
  # top_k: 0.0
  top_p: 0.95
  num_beams: 1
  temperature: 0.6
  do_sample: true
  #pad_token_id: 0
  bos_token_id: 128000
  eos_token_id: 128001
  max_new_tokens: 700
  return_prompt: False
  generate_ref_response: False
  use_cache : True
