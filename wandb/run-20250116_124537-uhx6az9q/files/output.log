`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|            | 0/4 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/oso/paper_camb/mars-steg/mars_steg/train_dataset_task.py", line 51, in <module>
    model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16,quantization_config=quantization_config,)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/modeling_utils.py", line 944, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 238, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 331, in to
    return self._quantize(device)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 296, in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/bitsandbytes/functional.py", line 1211, in quantize_4bit
    out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 30.19 MiB is free. Process 398924 has 21.00 GiB memory in use. Including non-PyTorch memory, this process has 2.40 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 9.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/oso/paper_camb/mars-steg/mars_steg/train_dataset_task.py", line 51, in <module>
    model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16,quantization_config=quantization_config,)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/modeling_utils.py", line 944, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 238, in create_quantized_param
    new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 331, in to
    return self._quantize(device)
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 296, in _quantize
    w_4bit, quant_state = bnb.functional.quantize_4bit(
  File "/home/oso/anaconda3/envs/stego/lib/python3.10/site-packages/bitsandbytes/functional.py", line 1211, in quantize_4bit
    out = torch.zeros(((n + 1) // mod, 1), dtype=quant_storage, device=A.device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 30.19 MiB is free. Process 398924 has 21.00 GiB memory in use. Including non-PyTorch memory, this process has 2.40 GiB memory in use. Of the allocated memory 1.96 GiB is allocated by PyTorch, and 9.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
